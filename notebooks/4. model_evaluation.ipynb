{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65488c23",
   "metadata": {},
   "source": [
    "### k-fold cross validation for above methods after applying tuning techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "fe390356",
   "metadata": {},
   "outputs": [],
   "source": [
    "z=data_bal.to_numpy()\n",
    "z_nb=data_NB.to_numpy()\n",
    "z_knn=data_KNN.to_numpy()\n",
    "n=len(data_bal)\n",
    "n_nb=len(data_NB)\n",
    "n_knn=len(data_KNN)\n",
    "m=len(data_bal.columns)\n",
    "m_nb=len(data_NB.columns)\n",
    "m_knn=len(data_KNN.columns)\n",
    "y=z[:,m-1:m]\n",
    "y_nb=z_nb[:,m-1:m]\n",
    "y_knn=z_knn[:,m-1:m]\n",
    "x=z[0:n,1:m-1]\n",
    "x_nb=z_nb[0:n,1:m-1]\n",
    "x_knn=z_knn[0:n,1:m-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3b9edce7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy of decision tree after tuning using 5- fold cross validation is [0.7335083688874302, 0.7896291434197571, 0.9461765671151953, 0.8447653429602888, 0.7401445466491459] and the avg is 0.8108447938063634 .\n",
      "Testing accuracy of decision tree after tuning using 5 fold cross validation is [0.5532194480946123, 0.568988173455979, 0.771353482260184, 0.7332457293035479, 0.5366492146596858] and the average is 0.6326912095548018 .\n",
      "\n",
      "\n",
      "Training accuracy of random forest after tuning using 5- fold cross validation is [0.8667541844437151, 0.9967180833606827, 0.9878569084345258, 0.9675090252707581, 0.8012483574244416] and the avg is 0.9240173117868247 .\n",
      "Testing accuracy of random forest after tuning using 5 fold cross validation is [0.5584756898817346, 0.6084099868593955, 0.8383705650459922, 0.8042049934296978, 0.5366492146596858] and the average is 0.6692220899753012 .\n",
      "\n",
      "\n",
      "The optimal k for KNN is 5 .\n",
      "Training accuracy of KNN after tuning using 5- fold cross validation is [0.9894978667541845, 0.9862159501148671, 0.9835904168034132, 0.9842468001312766, 0.9908015768725361] and the avg is 0.9868705221352555 .\n",
      "Testing accuracy of KNN after tuning using 5 fold cross validation is [0.9763469119579501, 0.9894875164257556, 1.0, 0.9973718791064389, 0.9712041884816754] and the average is 0.986882099194364 .\n",
      "\n",
      "\n",
      "Training accuracy of Naive Bayes after tuning using 5- fold cross validation is [0.5518867924528302, 0.4834905660377358, 0.5448113207547169, 0.5448113207547169, 0.4215456674473068] and the avg is 0.5093091334894614 .\n",
      "Testing accuracy of Naive Bayes after tuning using 5 fold cross validation is [0.5226603884638023, 0.531489111241907, 0.44791053560918187, 0.5256032960565038, 0.5341981132075472] and the average is 0.5123722889157885 .\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score,GridSearchCV\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "#k-fold cross validation\n",
    "i=5\n",
    "n_k=n//i\n",
    "n_k_nb=n_nb//i\n",
    "n_k_knn=n_knn//i\n",
    "\n",
    "train_acc_dtp=[]\n",
    "train_acc_rft=[]\n",
    "train_acc_knnt=[]\n",
    "train_acc_nbc= []\n",
    "acc_dtp=[]\n",
    "acc_rft=[]\n",
    "acc_knnt=[]\n",
    "acc_nbc= []\n",
    "\n",
    "#Hyper parameter tuning for selecting best k in KNN\n",
    "neighbors = [p for p in range(2,100) if p % 2 != 0]\n",
    "cv_scores = []\n",
    "for k in neighbors:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    yt=np.ravel(y_knn)\n",
    "    scores = cross_val_score(knn, x_knn, yt, cv=5, scoring='accuracy')\n",
    "    cv_scores.append(scores.mean())\n",
    "#misclassification error\n",
    "MSE = [1-t for t in cv_scores]\n",
    "#optimal K\n",
    "best_k_index = MSE.index(min(MSE))\n",
    "best_k = neighbors[best_k_index]\n",
    "\n",
    "for k in range(0,5):\n",
    "    if (k==4):\n",
    "        x_train=x[:n_k*k,:]\n",
    "        x_train_nb=x_nb[:n_k_nb*k,:]\n",
    "        x_train_knn=x_knn[:n_k_knn*k,:]\n",
    "        y_train=y[:n_k*k,:]\n",
    "        y_train_nb=y_nb[:n_k_nb*k,:]\n",
    "        y_train_knn=y_knn[:n_k_knn*k,:]\n",
    "        x_test=x[n_k*k:,:]\n",
    "        x_test_nb=x_nb[n_k_nb*k:,:]\n",
    "        x_test_knn=x_knn[n_k_knn*k:,:]\n",
    "        y_test=y[n_k*k:,:]\n",
    "        y_test_nb=y_nb[n_k_nb*k:,:]\n",
    "        y_test_knn=y_knn[n_k_knn*k:,:]\n",
    "    else:\n",
    "        x_train=np.delete(x,slice(n_k*k,n_k*(k+1)),0)\n",
    "        x_train_nb=np.delete(x_nb,slice(n_k_nb*k,n_k_nb*(k+1)),0)\n",
    "        x_train_knn=np.delete(x_knn,slice(n_k_knn*k,n_k_knn*(k+1)),0)\n",
    "        y_train=np.delete(y,slice(n_k*k,n_k*(k+1)),0)\n",
    "        y_train_nb=np.delete(y_nb,slice(n_k_nb*k,n_k_nb*(k+1)),0)\n",
    "        y_train_knn=np.delete(y_knn,slice(n_k_knn*k,n_k_knn*(k+1)),0)\n",
    "        x_test=x[n_k*k:n_k*(k+1),:]\n",
    "        x_test_nb=x_nb[n_k_nb*k:n_k_nb*(k+1),:]\n",
    "        x_test_knn=x_knn[n_k_knn*k:n_k_knn*(k+1),:]\n",
    "        y_test=y[n_k*k:n_k*(k+1),:]\n",
    "        y_test_nb=y_nb[n_k_nb*k:n_k_nb*(k+1),:]\n",
    "        y_test_knn=y_knn[n_k_knn*k:n_k_knn*(k+1),:]\n",
    "        \n",
    "#     #Feature Scaling\n",
    "#     sc= StandardScaler()\n",
    "#     x_train = sc.fit_transform(x_train)\n",
    "#     x_test = sc.transform(x_test)\n",
    "   \n",
    "    #Decision tree with pruning\n",
    "    decision_t=tree.DecisionTreeClassifier(random_state=42)\n",
    "    path = decision_t.cost_complexity_pruning_path(x_train,y_train)\n",
    "    alphas = path['ccp_alphas']\n",
    "    acc_train= []\n",
    "    acc_test = []\n",
    "    for j in alphas:\n",
    "        decision_t=tree.DecisionTreeClassifier(random_state=42,ccp_alpha=j)\n",
    "        decision_t.fit(x_train,y_train)\n",
    "        y_train_pred = decision_t.predict(x_train)\n",
    "        y_test_pred = decision_t.predict(x_test)\n",
    "        acc_train.append(metrics.accuracy_score(y_train,y_train_pred))\n",
    "        acc_test.append(metrics.accuracy_score(y_test,y_test_pred))\n",
    "    max_value = max(acc_test)\n",
    "    index = acc_test.index(max_value)\n",
    "    decision_t=tree.DecisionTreeClassifier(random_state=42, ccp_alpha=alphas[index])\n",
    "    decision_t.fit(x_train,y_train)\n",
    "    #train accuracy\n",
    "    y_train_pred = decision_t.predict(x_train)\n",
    "    y_test_pred = decision_t.predict(x_test)\n",
    "    train_acc = metrics.accuracy_score(y_train,y_train_pred)\n",
    "    train_acc_dtp.append(train_acc)\n",
    "    #test accuracy\n",
    "    accuracy=metrics.accuracy_score(y_test,y_test_pred)\n",
    "    acc_dtp.append(accuracy)\n",
    "    \n",
    "    \n",
    "    #Random Forest max_depth tuning\n",
    "    val_acc = []\n",
    "    tr_acc = []\n",
    "    dep = []\n",
    "    for depth in range(2, 100, 2): \n",
    "        classifier = RandomForestClassifier(n_estimators=25, max_depth = depth, random_state=42)\n",
    "        y_trainm=np.ravel(y_train)\n",
    "        classifier.fit(x_train, y_trainm)\n",
    "        #print(\"For depth = \", str(depth))\n",
    "        #print(\"Training done\")\n",
    "        #print(\"Training accuracy: \", classifier.score(x_train, y_train))\n",
    "        #print(\"Test set accuracy: \", classifier.score(x_test, y_test))\n",
    "        val_acc.append(classifier.score(x_test, y_test))\n",
    "        tr_acc.append(classifier.score(x_train, y_train))\n",
    "        dep.append(depth)\n",
    "    max_value = max(val_acc)\n",
    "    index = val_acc.index(max_value)\n",
    "    classifier=RandomForestClassifier(n_estimators=25, max_depth = dep[index], random_state=42)\n",
    "    y_train=np.ravel(y_train)\n",
    "    classifier.fit(x_train,y_train)\n",
    "    #train accuracy\n",
    "    y_trainpred1 = classifier.predict(x_train)\n",
    "    train_acc1 = metrics.accuracy_score(y_train,y_trainpred1)\n",
    "    train_acc_rft.append(train_acc1)\n",
    "    #test accuracy\n",
    "    y_pred1=classifier.predict(x_test)\n",
    "    acc=metrics.accuracy_score(y_test,y_pred1)\n",
    "    acc_rft.append(acc)\n",
    "    \n",
    "    \n",
    "   \n",
    "    #Hyper parameter tuning for selecting best k in KNN\n",
    "    classifier = KNeighborsClassifier(n_neighbors = best_k)\n",
    "    y_train_knn = np.ravel(y_train_knn)\n",
    "    classifier.fit(x_train_knn,y_train_knn)\n",
    "    #train accuracy\n",
    "    y_trainpred2 = classifier.predict(x_train_knn)\n",
    "    train_acc2 = metrics.accuracy_score(y_train_knn,y_trainpred2)\n",
    "    train_acc_knnt.append(train_acc2)\n",
    "    #test accuracy\n",
    "    y_pred2 = classifier.predict(x_test_knn)\n",
    "    acc1 =metrics.accuracy_score(y_test_knn,y_pred2)\n",
    "    acc_knnt.append(acc1)\n",
    "    \n",
    "    #Hyper parameter tuninf for Naive bayes Classifier \n",
    "    classifier = GaussianNB()\n",
    "    np.logspace(0,-9, num=10)\n",
    "    cv_method = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=999);\n",
    "    params_NB = {'var_smoothing': np.logspace(0,-9, num=100)}\n",
    "    gs_NB = GridSearchCV(estimator=classifier, param_grid=params_NB, cv=cv_method, verbose=0, scoring='accuracy');\n",
    "    #training accuracy\n",
    "    data_transformed = PowerTransformer().fit_transform(x_train_nb);\n",
    "    gs_NB.fit(data_transformed, y_train_nb);\n",
    "    y_trainpred3 = gs_NB.predict(x_train_nb)\n",
    "    train_acc3 = metrics.accuracy_score(y_train_nb,y_trainpred3)\n",
    "    acc_nbc.append(train_acc3)\n",
    "    #test accuracy\n",
    "    data_transformed = PowerTransformer().fit_transform(x_test_nb);\n",
    "    gs_NB.fit(data_transformed, y_test_nb);\n",
    "    y_pred3 = gs_NB.predict(x_test_nb)\n",
    "    acc2 =metrics.accuracy_score(y_test_nb,y_pred3)\n",
    "    train_acc_nbc.append(acc2)\n",
    "    \n",
    "    \n",
    "\n",
    "avg_acc_dtp=mean(acc_dtp) \n",
    "avg_acc_rft=mean(acc_rft)\n",
    "avg_acc_knnt=mean(acc_knnt)\n",
    "\n",
    "avg_trainacc_dtp=mean(train_acc_dtp) \n",
    "avg_trainacc_rft=mean(train_acc_rft) \n",
    "avg_trainacc_knnt=mean(train_acc_knnt)\n",
    "\n",
    "avg_acc_nbc=mean(acc_nbc)\n",
    "avg_trainacc_nbc=mean(train_acc_nbc)\n",
    "\n",
    "\n",
    "\n",
    "print(\"Training accuracy of decision tree after tuning using 5- fold cross validation is\",train_acc_dtp,\"and the avg is\",avg_trainacc_dtp,\".\")\n",
    "print(\"Testing accuracy of decision tree after tuning using 5 fold cross validation is\",acc_dtp,\"and the average is\",avg_acc_dtp,\".\")\n",
    "print(\"\\n\")\n",
    "print(\"Training accuracy of random forest after tuning using 5- fold cross validation is\",train_acc_rft,\"and the avg is\",avg_trainacc_rft,\".\")\n",
    "print(\"Testing accuracy of random forest after tuning using 5 fold cross validation is\",acc_rft,\"and the average is\",avg_acc_rft,\".\")\n",
    "print(\"\\n\")\n",
    "print(\"The optimal k for KNN is\", best_k,\".\")\n",
    "print(\"Training accuracy of KNN after tuning using 5- fold cross validation is\",train_acc_knnt,\"and the avg is\",avg_trainacc_knnt,\".\")\n",
    "print(\"Testing accuracy of KNN after tuning using 5 fold cross validation is\",acc_knnt,\"and the average is\",avg_acc_knnt,\".\")\n",
    "print(\"\\n\")\n",
    "print(\"Training accuracy of Naive Bayes after tuning using 5- fold cross validation is\",train_acc_nbc,\"and the avg is\",avg_trainacc_nbc,\".\")\n",
    "print(\"Testing accuracy of Naive Bayes after tuning using 5 fold cross validation is\",acc_nbc,\"and the average is\",avg_acc_nbc,\".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa48362",
   "metadata": {},
   "source": [
    "We observe that KNN performs the best among all."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
